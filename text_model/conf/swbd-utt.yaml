print_dims: False
print_every: 50
eval_every: 500
train_ratio: 0.6
dev_ratio: 0.2

model_type: 'lstm'

batch_size: 64
bidirectional: True
learning_rate: 0.001
embedding_dim: 100
hidden_size: 128
use_pretrained: false
max_len: 80
datasource: 'SWBDNXT_UTT'
vocab_size: 4000
num_epochs: 18
num_layers: 2
dropout: 0.8

datafile: '../data/utterances_text_only.txt'
glove_path: '../data/emb/glove.6B.100d.txt'
#model_name: 'nonseq-utt-l2-b64-d8-e19-v4000-lr001'
model_name: 'tmp'
results_path: 'turn_utt_investigation'