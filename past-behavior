Before training, evaluation on train:
Evaluation:
F1: 0.06256306760847628
Acc 0.49233439895500325
           precision    recall  f1-score   support

        1       0.07      0.05      0.06     16243
        0       0.10      0.05      0.07     16030

micro avg       0.08      0.05      0.06     32273
macro avg       0.09      0.05      0.06     32273

Before training, evaluation on dev:
Evaluation:
F1: 0.08133172052301135
Acc 0.49059458369055425
           precision    recall  f1-score   support

        1       0.13      0.09      0.10      5468
        0       0.08      0.04      0.06      5370

micro avg       0.11      0.07      0.08     10838
macro avg       0.10      0.07      0.08     10838

=======================================================

Epoch: 0 Step: 1 Loss: 1.3446592092514038
Epoch: 0 Step: 1001 Loss: 0.6979738473892212
Epoch: 0 Step: 2001 Loss: 0.6954843401908875
Epoch: 0 Step: 3001 Loss: 0.6946180462837219
Epoch: 0 Step: 4001 Loss: 0.6943312883377075

=======================================================

Evaluation on dev after 1 step:
F1: 0.07868852459016393
Acc 0.5058752332442382
           precision    recall  f1-score   support

        1       0.14      0.07      0.10      5468
        0       0.11      0.04      0.06      5370

micro avg       0.13      0.06      0.08     10838
macro avg       0.13      0.06      0.08     10838


Evaluation at 2001 steps:
F1: 0.09772646716289433
Acc 0.49089717081042916
           precision    recall  f1-score   support

        1       0.11      0.08      0.09      5468
        0       0.11      0.10      0.10      5370

micro avg       0.11      0.09      0.10     10838
macro avg       0.11      0.09      0.10     10838

Evaluation at 4001 steps:
F1: 0.07660707053173639
Acc 0.4885773362247214
           precision    recall  f1-score   support

        1       0.11      0.05      0.07      5468
        0       0.10      0.07      0.08      5370

micro avg       0.10      0.06      0.08     10838
macro avg       0.10      0.06      0.08     10838

=======================================================

After training, evaluation on train:
Evaluation:
F1: 0.07610142186733909
Acc 0.5091093465332921
           precision    recall  f1-score   support

        1       0.09      0.07      0.08     16243
        0       0.09      0.07      0.08     16030

micro avg       0.09      0.07      0.08     32273
macro avg       0.09      0.07      0.08     32273

After training, evaluation on dev: 
Evaluation:
F1: 0.10224388262767795
Acc 0.48721569418528415
           precision    recall  f1-score   support

        1       0.11      0.09      0.10      5468
        0       0.12      0.10      0.11      5370

micro avg       0.11      0.09      0.10     10838
macro avg       0.11      0.09      0.10     10838


=======================================================
THREE EPOCHS
=======================================================

Before training, train:
Evaluation:
F1: 0.06508581835574606
Acc 0.4905640919872125
           precision    recall  f1-score   support

        0       0.08      0.05      0.07     16030
        1       0.08      0.06      0.06     16243

micro avg       0.08      0.05      0.07     32273
macro avg       0.08      0.05      0.07     32273

Before training, dev:
Evaluation:
F1: 0.08933274802458299
Acc 0.5001260779666146
           precision    recall  f1-score   support

        1       0.12      0.08      0.10      5468
        0       0.10      0.07      0.08      5370

micro avg       0.11      0.08      0.09     10838
macro avg       0.11      0.08      0.09     10838


=======================================================

Epoch: 0 Step: 1 Loss: 1.3253843784332275
Epoch: 0 Step: 1001 Loss: 0.6981431841850281
Epoch: 0 Step: 2001 Loss: 0.6956540942192078
Epoch: 0 Step: 3001 Loss: 0.694800853729248
Epoch: 0 Step: 4001 Loss: 0.6947866678237915
Epoch: 1 Step: 1    Loss: 0.6947914361953735
Epoch: 1 Step: 1001 Loss: 0.694954514503479 <<<<< loss goes up
Epoch: 1 Step: 2001 Loss: 0.6947222948074341
Epoch: 1 Step: 3001 Loss: 0.6945235729217529
Epoch: 1 Step: 4001 Loss: 0.6945265531539917
Epoch: 2 Step: 1    Loss: 0.694541871547699
Epoch: 2 Step: 1001 Loss: 0.6946355104446411
Epoch: 2 Step: 2001 Loss: 0.6945111155509949
Epoch: 2 Step: 3001 Loss: 0.6944024562835693
Epoch: 2 Step: 4001 Loss: 0.6943990588188171 << This is log2

=======================================================

Evaluation Epoch: 0 Step: 1:
F1: 0.07849476270528902
Acc 0.4971506379545111
           precision    recall  f1-score   support

        1       0.12      0.06      0.08      5468
        0       0.14      0.05      0.08      5370

micro avg       0.13      0.06      0.08     10838
macro avg       0.13      0.06      0.08     10838


Evaluation Epoch: 0 Step: 2001:
F1: 0.09571175950486294
Acc 0.4911997579303041
           precision    recall  f1-score   support

        1       0.13      0.08      0.10      5468
        0       0.11      0.08      0.09      5370

micro avg       0.12      0.08      0.10     10838
macro avg       0.12      0.08      0.10     10838


Evaluation Epoch: 0 Step: 4001:
F1: 0.09949380345610054
Acc 0.5078420495234253
           precision    recall  f1-score   support

        1       0.17      0.09      0.12      5468
        0       0.11      0.07      0.08      5370

micro avg       0.13      0.08      0.10     10838
macro avg       0.14      0.08      0.10     10838


Evaluation Epoch: 1 Step: 1:
F1: 0.05721466587817859 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< F1 drops
Acc 0.49432649150234503
           precision    recall  f1-score   support

        1       0.15      0.05      0.08      5468
        0       0.14      0.02      0.03      5370

micro avg       0.14      0.04      0.06     10838
macro avg       0.14      0.04      0.06     10838


Evaluation Epoch: 1 Step: 2001:
F1: 0.09934374292826431
Acc 0.4964950325281154
           precision    recall  f1-score   support

        1       0.14      0.08      0.10      5468
        0       0.12      0.08      0.10      5370

micro avg       0.13      0.08      0.10     10838
macro avg       0.13      0.08      0.10     10838


Evaluation Epoch: 1 Step: 4001:
F1: 0.08390671619070816
Acc 0.4985627111805941
           precision    recall  f1-score   support

        1       0.14      0.06      0.08      5468
        0       0.11      0.07      0.09      5370

micro avg       0.12      0.06      0.08     10838
macro avg       0.13      0.06      0.08     10838


Evaluation Epoch: 2 Step: 1:
F1: 0.04969914823786826 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< F1 drops
Acc 0.49296484946290786
           precision    recall  f1-score   support

        1       0.17      0.05      0.08      5468
        0       0.14      0.01      0.01      5370

micro avg       0.16      0.03      0.05     10838
macro avg       0.15      0.03      0.05     10838

Evaluation Epoch: 2 Step: 2001:
F1: 0.08406472028180786
Acc 0.49548640879519895
           precision    recall  f1-score   support

        1       0.13      0.06      0.08      5468
        0       0.11      0.07      0.09      5370

micro avg       0.12      0.06      0.08     10838
macro avg       0.12      0.06      0.08     10838


Evaluation Epoch: 2 Step: 4001:
F1: 0.07814083080040526
Acc 0.49866357355388574
           precision    recall  f1-score   support

        1       0.14      0.05      0.07      5468
        0       0.11      0.06      0.08      5370

micro avg       0.12      0.06      0.08     10838
macro avg       0.13      0.06      0.08     10838

=======================================================


After training, train:
Evaluation:
F1: 0.04647666502008118
Acc 0.48241724244611733
           precision    recall  f1-score   support

        0       0.12      0.04      0.06     16030
        1       0.05      0.02      0.03     16243

micro avg       0.08      0.03      0.05     32273
macro avg       0.08      0.03      0.05     32273

After training, dev: 
Evaluation:
F1: 0.0780506377308205
Acc 0.4969993443945736
           precision    recall  f1-score   support

        1       0.10      0.05      0.07      5468
        0       0.16      0.06      0.09      5370

micro avg       0.12      0.06      0.08     10838
macro avg       0.13      0.06      0.08     10838


TO DO:

Print model parameters (.namedparameters())
Why does the loss start out big?
Initialized weights -- check that they got initialized sensibly (they should have)
Detach the loss
Do model.zero_grad() before backward()
Use Adam defaults to start with
Gradients are noisy right now, so possibly lower learning rate
DON'T use view to reshape from nxm to mxn. 
Output of linear layer can be 1, then use binary cross entropy
Refresh loss every epoch OR keep a stack of losses



