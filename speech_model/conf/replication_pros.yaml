# dataloader params
segmentation: tokens
context_window: True
datasplit: ../data/burnc/splits/tenfold0.yaml
feats: tok2pros
bitmark: True

# Debugging params
VERBOSE: False
LENGTH_ANALYSIS: False
print_every: 500
eval_every:

# i/o params
all_data: '../data/burnc/burnc.pkl'
model_name: 'replication_text'
results_path: 'results/refactor/replication'

train_params:
  batch_size: 32
  shuffle: True
  num_workers: 6

eval_params:
  batch_size: 32
  shuffle: True
  num_workers: 6

# model params
tok_level_pred: False
include_lstm: False
bidirectional: False
learning_rate: 0.001
hidden_size: 512
frame_pad_len: 420
tok_pad_len: 3
# if no context, smaller pad length:
#frame_pad_len: 373
datasource: 'BURNC'
num_epochs: 20
cnn_layers: 3
lstm_layers: 2
dropout: 0.2
feat_dim: 7
#feat_dim: 6
postlstm_context: False
weight_decay: 0.001
frame_filter_size: 15
flatten_method: sum
frame_pad_size: 7

seed: 131

# Text-specific params
inputs: both
embedding_dim: 300
use_pretrained: True
vocab_size: 3000
glove_path: '../data/emb/glove.6B.100d.txt'
bottleneck_feats: 1600



