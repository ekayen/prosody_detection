{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "#### Current format: \n",
    "Tokens each have an xml entry in their phonwords file. \n",
    "Accents each have an xml entry in their accents file.\n",
    "Turns have are in their own files, and are recorded as time spans. The time span of a phonword is somewhere inside the time span of a turn.\n",
    "\n",
    "Note: Each file is per-speaker, not per-turn or per-conversation.\n",
    "\n",
    "#### Desired format:\n",
    "One turn per line, whitespace-separated tokens, tab, binary string with 1 for nuclear pitch accent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#data_dir = '/afs/inf.ed.ac.uk/group/corpora/large/switchboard/nxt/xml'\n",
    "data_dir = '/home/ekayen/repos/stars/nxt-subset'\n",
    "\n",
    "dialog_num = 'sw2018'\n",
    "users = ('A','B')\n",
    "turn_files = [os.path.join(data_dir,'turns','.'.join([dialog_num,user,'turns','xml'])) for user in users]\n",
    "acc_files = [os.path.join(data_dir,'accent','.'.join([dialog_num,user,'accents','xml'])) for user in users]\n",
    "wd_files = [os.path.join(data_dir,'phonwords','.'.join([dialog_num,user,'phonwords','xml'])) for user in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "nite = '{http://nite.sourceforge.net/}'\n",
    "\n",
    "accent_dict = {'nuclear':1,\n",
    "               'plain':0,\n",
    "               'pre-nuclear':0}\n",
    "\n",
    "words = []\n",
    "ids = []\n",
    "times = []\n",
    "wd_to_i = {}\n",
    "i_to_wd = {}\n",
    "id_to_acc = {}\n",
    "counter = 0\n",
    "for i,wd_file in enumerate(wd_files):\n",
    "    tmp_wds = []\n",
    "    tmp_ids = []\n",
    "    tmp_times = []\n",
    "    if os.path.exists(acc_files[i]): # only pay attention to ones that have accent files\n",
    "        wd_tree = ET.parse(wd_file)\n",
    "        wd_root = wd_tree.getroot()\n",
    "        for phonword in wd_root.findall('phonword'):\n",
    "            orth = phonword.attrib['orth']\n",
    "            id_num = phonword.attrib[nite+'id']\n",
    "            start_time = float(phonword.attrib[nite+'start'])\n",
    "            if not orth in wd_to_i:\n",
    "                wd_to_i[orth] = counter\n",
    "                i_to_wd[counter] = orth\n",
    "                counter += 1\n",
    "            tmp_wds.append(wd_to_i[orth])\n",
    "            tmp_ids.append(id_num) # TODO since these ids are unique, I can make a lookup table for them too for speed\n",
    "            tmp_times.append(start_time)\n",
    "        words.append(tmp_wds)\n",
    "        ids.append(tmp_ids)\n",
    "        times.append(tmp_times)\n",
    "                \n",
    "        acc_tree = ET.parse(acc_files[i])\n",
    "        acc_root = acc_tree.getroot()\n",
    "        for acc in acc_root.findall('accent'):\n",
    "            for chld in acc:\n",
    "                acc_id = chld.attrib['href'].split('(')[-1][:-1]\n",
    "                id_to_acc[acc_id]=accent_dict[acc.attrib['type']]\n",
    "    else:\n",
    "        print('no accent file found')\n",
    "        \n",
    "            \n",
    "#print([i_to_wd[i] for i in words[:5]])\n",
    "len(ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make np array of accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(856,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = (np.array(words[0]), np.array(words[1]))\n",
    "times = (np.array(times[0]), np.array(times[1]))\n",
    "words[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "accents = (np.zeros(words[0].shape,dtype=np.int32),np.zeros(words[1].shape,dtype=np.int32))\n",
    "\n",
    "for i in (0,1):\n",
    "    for j in range(words[i].shape[0]):\n",
    "        id_num = ids[i][j]\n",
    "        if id_num in id_to_acc:\n",
    "            accents[i][j] = id_to_acc[id_num]\n",
    "        else:\n",
    "            accents[i][j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through turns, writing the final form out turn by turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns0 = [(int(child.attrib[nite+'id'][1:]), (float(child.attrib[nite+'start']), float(child.attrib[nite+'end']),0)) for child in ET.parse(turn_files[0]).getroot()]\n",
    "turns1 = [(int(child.attrib[nite+'id'][1:]), (float(child.attrib[nite+'start']), float(child.attrib[nite+'end']),1)) for child in ET.parse(turn_files[1]).getroot()]\n",
    "turns = turns0 + turns1\n",
    "turns.sort()\n",
    "turns = dict(turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for num in turns:\n",
    "    speaker = turns[num][2]\n",
    "    start = turns[num][0]\n",
    "    end = turns[num][1]\n",
    "    mask = np.squeeze(np.logical_and([times[speaker] >= start], [times[speaker] <= end]))\n",
    "    turn = words[speaker][mask]\n",
    "    acc = accents[speaker][mask]\n",
    "    tokens = ' '.join([i_to_wd[i] for i in turn])\n",
    "    labels = ' '.join([str(i) for i in acc])\n",
    "    lines.append((tokens,labels))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('lines.pickle','wb') as f:\n",
    "    pickle.dump(lines,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also write them to a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lines.txt','w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line[0]+'\\t'+str(line[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
