{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data by utterance, not by turn\n",
    "\n",
    "The original SWBD corpus organizes its annotation by utterance; the SWBD NXT corpus uses turns. I have been using turns heretofore, but since the audio is organized by the original Switchboard system, I'm going to have to switch to using utterances. \n",
    "\n",
    "Utterances are also prevented from being too long, and so will likely be easier to process when using audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:\n",
    "\n",
    "Find conversation numbers in SWBD NXT. SWBD NXT is a subset of SWBD, so we only want to find Kaldi feats for conversations that are in both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "NITE = '{http://nite.sourceforge.net/}'\n",
    "PHON_DIR = '/afs/inf.ed.ac.uk/group/corpora/large/switchboard/nxt/xml/phonwords'\n",
    "ACC_DIR = '/afs/inf.ed.ac.uk/group/corpora/large/switchboard/nxt/xml/accent'\n",
    "SPEAKERS = ['A','B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sw2018', 'sw2060', 'sw2107']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convs = set()\n",
    "\n",
    "for filename in os.listdir(ACC_DIR):\n",
    "    convs.add(filename.split('.')[0])\n",
    "\n",
    "convs = sorted(list(convs))\n",
    "convs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: \n",
    "\n",
    "For each conversation, iterate through phonwords, separating by the 'msstate' field (which corresponds to the utterance). Store in a dictionary, where key = conversation num + utterance num, value = list of tuples of (phonword, accent) or (phonword, id) (haven't decided which yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_orth(orth):\n",
    "    mispron_dict = {\n",
    "        '[row/wow]': 'wow',\n",
    "        '[mot/lot]': 'lot',\n",
    "        '[trest/test]': 'test',\n",
    "        '[adamnet/adapted]': 'adapted',\n",
    "        '[storly/story]': 'story',\n",
    "        '[unconvenient/inconvenient]': 'inconvenient',\n",
    "        '[dib/bit]': 'bit',\n",
    "        '[tack/talking]': 'talking',\n",
    "        '[banding/banning]': 'banning',\n",
    "        '[ruther/rather]': 'rather',\n",
    "        '[shrip/shrimp]': 'shrimp',\n",
    "    }\n",
    "\n",
    "    orth = orth.lower()\n",
    "    orth = orth.strip('-')\n",
    "    orth = orth.strip('{')\n",
    "    orth = orth.strip('}')\n",
    "    if '[laughter-' in orth:\n",
    "        orth = orth.replace('[laughter-', '')\n",
    "        orth = orth.replace(']', '')\n",
    "    if orth in mispron_dict:\n",
    "        orth = mispron_dict[orth]\n",
    "\n",
    "    return orth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = {}\n",
    "\n",
    "for conv in convs:\n",
    "    for sp in SPEAKERS:\n",
    "        acc_wds = set()\n",
    "        accfile = os.path.join(ACC_DIR,'.'.join([conv,sp,'accents','xml']))\n",
    "        accroot = ET.parse(accfile).getroot()\n",
    "        for acc in accroot.findall('accent'):\n",
    "            for child in acc:\n",
    "                wd_id = child.attrib['href'].split('(')[-1][:-1]\n",
    "                acc_wds.add(wd_id)\n",
    "        \n",
    "        phonfile = os.path.join(PHON_DIR,'.'.join([conv,sp,'phonwords','xml']))\n",
    "        phonroot = ET.parse(phonfile).getroot()\n",
    "        for phonword in phonroot.findall('phonword'):\n",
    "            orth = reg_orth(phonword.attrib['orth'])\n",
    "            wd = phonword.attrib[NITE+'id']\n",
    "            if wd in acc_wds:\n",
    "                acc = 1\n",
    "            else:\n",
    "                acc = 0\n",
    "            utt = phonword.attrib['msstate']\n",
    "            if not utt=='': # TODO Fix this to salvage stuff that doesn't have an msstate value\n",
    "                if not utt in utterances:\n",
    "                    utterances[utt] = [(orth,acc)]\n",
    "                else:\n",
    "                    utterances[utt].append((orth,acc))\n",
    "    \n",
    "#utterances            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Print out file for training\n",
    "\n",
    "Format for each line:\n",
    "\n",
    "`utterance_id <TAB> kaldi_handle <TAB> tokens <TAB> labels`\n",
    "    \n",
    "Kaldi handle is the key used in feats.scp. Slightly different formatting and uses timestamps instead of utterance number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sw02018-A_017785-017911'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swbd_dir = '/afs/inf.ed.ac.uk/group/corpora/large/switchboard/switchboard1/transcriptions/swb_ms98_transcriptions'\n",
    "\n",
    "def kaldify_utt(utt):\n",
    "    subdir = utt[2:4]\n",
    "    conversation = utt[2:6]\n",
    "    conv_sp = utt.split('-')[0]\n",
    "    sp = conv_sp[-1]\n",
    "    \n",
    "    filename = '-'.join([conv_sp,'ms98','a','trans'])+'.text'\n",
    "    transcript_path = os.path.join(swbd_dir,subdir,conversation,filename)\n",
    "    time_dict = {}\n",
    "    with open(transcript_path,'r') as f:\n",
    "        lines = [line.split()[:3] for line in f.readlines()]\n",
    "        for line in lines:\n",
    "            time_dict[line[0]] = (line[1],line[2])     \n",
    "    start = kaldify_time(time_dict[utt][0])\n",
    "    end = kaldify_time(time_dict[utt][1])\n",
    "    kaldi_handle = 'sw0'+conversation+'-'+sp+'_'+start+'-'+end\n",
    "    return kaldi_handle\n",
    "\n",
    "def kaldify_time(time):\n",
    "    time = float(time)\n",
    "    time = str(round(time*100))\n",
    "    pad_num = 6-len(time)\n",
    "    time = ('0'*pad_num)+time\n",
    "    return time\n",
    "    \n",
    "\n",
    "kaldify_utt('sw2018A-ms98-a-0037')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data/utterances.txt'\n",
    "\n",
    "with open(datafile,'w') as f:\n",
    "    for utt in utterances:\n",
    "        tokens = [tup[0] for tup in utterances[utt]]\n",
    "        labels = [str(tup[1]) for tup in utterances[utt]]\n",
    "        kaldi_handle = kaldify_utt(utt)\n",
    "        f.write(utt+'\\t'+kaldi_handle+'\\t'+' '.join(tokens)+'\\t'+' '.join(labels)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data/utterances_text_only.txt'\n",
    "\n",
    "with open(datafile,'w') as f:\n",
    "    for utt in utterances:\n",
    "        tokens = [tup[0] for tup in utterances[utt]]\n",
    "        labels = [str(tup[1]) for tup in utterances[utt]]\n",
    "        kaldi_handle = kaldify_utt(utt)\n",
    "        f.write(' '.join(tokens)+'\\t'+' '.join(labels)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
